{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dataset.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM8pdZ01WjAE7QFRBCb0Uvx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aishik11/DeL-haTE/blob/master/dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sygunPq-cHcF"
      },
      "source": [
        "#!/usr/bin/env python3\n",
        "\n",
        "import warnings\n",
        "from collections import defaultdict\n",
        "from random import sample\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, Subset\n",
        "\n",
        "\n",
        "class PostDataset(Dataset):\n",
        "    def __init__(self, posts, embedding, labeled=True, padded_seq='max'):\n",
        "        self.posts = posts\n",
        "        self.post_ids = list(posts.keys())\n",
        "        self.labeled = labeled\n",
        "\n",
        "        self.embedding = embedding\n",
        "\n",
        "        self.max_seq_length = max(len(self.posts[ID]['tokens']) for ID in self.posts)\n",
        "\n",
        "        self.padded_seq = None\n",
        "        self.set_padding(padded_seq)\n",
        "\n",
        "        if self.labeled:\n",
        "            self.n_classes = max(self.posts[ID]['label'] for ID in self.posts) + 1\n",
        "\n",
        "            self.label_idx = defaultdict(list)  # Dict of post ids per class\n",
        "            for i, post in enumerate(self.post_ids):\n",
        "                self.label_idx[self.posts[post]['label']].append(i)\n",
        "        else:\n",
        "            self.n_classes = len(self.posts[self.post_ids[0]]['bounds'])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.posts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        ID = self.post_ids[idx]\n",
        "\n",
        "        # Get embeddings vectors from word tokens\n",
        "        X = self.embedding.get_vecs_by_tokens(self.posts[ID]['tokens'])\n",
        "\n",
        "        # Add zero left padding to standardize word embedding matrix size\n",
        "        X = F.pad(X, [0, 0, (self.padded_seq - X.shape[0]), 0])\n",
        "\n",
        "        if self.labeled:\n",
        "            y = self.posts[ID]['label']\n",
        "        else:\n",
        "            y = torch.tensor(self.posts[ID]['bounds'], dtype=torch.float)\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def set_padding(self, pad):\n",
        "        if pad == 'max':\n",
        "            pad = self.max_seq_length\n",
        "        else:\n",
        "            try:\n",
        "                pad = int(pad)\n",
        "            except TypeError as e:\n",
        "                raise Exception('Invalid padding. Padding must be an integer or '\n",
        "                                '\"max\" which will set padding to the length of max input sequence.') from e\n",
        "\n",
        "        self.padded_seq = pad\n",
        "\n",
        "    def sample_classes(self, n_samples=1000):\n",
        "        assert self.labeled\n",
        "\n",
        "        min_class = min(len(p_ids) for p_ids in self.label_idx.values())\n",
        "        if n_samples == 'all':\n",
        "            n_samples = min_class\n",
        "        elif n_samples > min_class:\n",
        "            warnings.warn(('Number of sample greater than examples of smallest class. '\n",
        "                           f'Setting n_samples = {min_class:,}'))\n",
        "            n_samples = min_class\n",
        "        elif type(n_samples) is not int:\n",
        "            raise TypeError('Invalid n_samples. n_samples must be an integer or '\n",
        "                            '\"all\" which will set n_samples to the number of examples in the smallest class.')\n",
        "\n",
        "        sample_idx = [\n",
        "            p_i\n",
        "            for p_indices in self.label_idx.values()\n",
        "            for p_i in sample(p_indices, k=n_samples)\n",
        "        ]\n",
        "\n",
        "        return Subset(self, sample_idx)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}