{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPoQXk/YG9gWNqtkkiX+mWQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aishik11/DeL-haTE/blob/master/model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQT1OVrwchjT"
      },
      "source": [
        "#!/usr/bin/env python3\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "from dataset import PostDataset\n",
        "\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from collections import OrderedDict, defaultdict\n",
        "\n",
        "\n",
        "class DelhateEnsemble(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            n_models,\n",
        "            seq_length,\n",
        "            embed_corpus,\n",
        "            embed_dim,\n",
        "            n_classes,\n",
        "            n_filters,\n",
        "            filter_width,\n",
        "            pool_size=4,\n",
        "            n_hidden=100,\n",
        "            rnn_type='gru',\n",
        "            dropout=0.2\n",
        "    ):\n",
        "        super(DelhateEnsemble, self).__init__()\n",
        "\n",
        "        self.n_models = n_models\n",
        "        self.seq_length = seq_length\n",
        "        self.embed_corpus = embed_corpus\n",
        "        self.embed_dim = embed_dim\n",
        "        self.n_classes = n_classes\n",
        "        self.n_filters = n_filters\n",
        "        self.filter_width = filter_width\n",
        "        self.pool_size = pool_size\n",
        "        self.n_hidden = n_hidden\n",
        "        self.dropout = dropout\n",
        "\n",
        "        if rnn_type is None or rnn_type.casefold() == 'none':\n",
        "            self.rnn_type = None\n",
        "        elif rnn_type.casefold() == 'lstm':\n",
        "            self.rnn_type = 'lstm'\n",
        "        elif rnn_type.casefold() == 'gru':\n",
        "            self.rnn_type = 'gru'\n",
        "        else:\n",
        "            raise ValueError('Invalid RNN type selected. Options are gru/lstm/None')\n",
        "\n",
        "        self.sub_models = nn.ModuleList()\n",
        "        for _ in range(self.n_models):\n",
        "            self.sub_models.append(\n",
        "                ComponentModel(\n",
        "                    self.seq_length,\n",
        "                    self.embed_corpus,\n",
        "                    self.embed_dim,\n",
        "                    self.n_classes,\n",
        "                    self.n_filters,\n",
        "                    self.filter_width,\n",
        "                    self.pool_size,\n",
        "                    self.n_hidden,\n",
        "                    self.rnn_type,\n",
        "                    self.dropout,\n",
        "                )\n",
        "            )\n",
        "\n",
        "    # TODO: Add reset_parameters() function\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Shape: batch size X num models X num classes\n",
        "        logits = torch.empty((x.shape[0], self.n_models, self.n_classes))\n",
        "        for i, model in enumerate(self.sub_models):\n",
        "            logits[:, i, :] = model(x.clone())\n",
        "\n",
        "        return logits\n",
        "\n",
        "    def evaluate(self, data, batch_size=64, device='cpu'):\n",
        "        data_loader = DataLoader(data, batch_size=batch_size)\n",
        "        y_pred, y_true = [], []\n",
        "\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            for X, y in data_loader:\n",
        "                X, y = X.to(device), y.to(device)\n",
        "\n",
        "                logits = self(X)\n",
        "                preds, _ = torch.softmax(logits, dim=2).argmax(dim=2).mode(dim=1)\n",
        "\n",
        "                y_pred.extend(preds.tolist())\n",
        "                y_true.extend(y.tolist())\n",
        "\n",
        "        return y_pred, y_true\n",
        "\n",
        "    def train_models(\n",
        "            self,\n",
        "            train_data,\n",
        "            loss_fn,\n",
        "            lr=1e-3,\n",
        "            n_samples=1000,\n",
        "            use_val=False,\n",
        "            early_stop=False,\n",
        "            batch_size=24,\n",
        "            EPOCHS=20,\n",
        "            device='cpu'\n",
        "    ):\n",
        "        # TODO: Parallelize?\n",
        "        losses = defaultdict(list)\n",
        "        self.to(device)\n",
        "        for i, model in enumerate(self.sub_models):\n",
        "            desc = f'Training model {i}'\n",
        "\n",
        "            optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "            val_data = None\n",
        "            if use_val or early_stop:\n",
        "                val_size = int(0.1 * len(train_data))\n",
        "                train_sub, val_data = random_split(train_data, [(len(train_data) - val_size), val_size])\n",
        "\n",
        "                # Create training Dataset for use class-balanced sampling\n",
        "                train_posts = {\n",
        "                    train_data.post_ids[p_id]: train_data.posts[train_data.post_ids[p_id]]\n",
        "                    for p_id in train_sub.indices\n",
        "                }\n",
        "\n",
        "                train_sub = PostDataset(posts=train_posts,\n",
        "                                        embedding=train_data.embedding,\n",
        "                                        labeled=train_data.labeled,\n",
        "                                        padded_seq=train_data.padded_seq)\n",
        "            else:\n",
        "                train_sub = train_data\n",
        "\n",
        "            t_loss, v_loss = model.train_model(train_sub,\n",
        "                                               loss_fn,\n",
        "                                               optimizer,\n",
        "                                               val_data,\n",
        "                                               n_samples,\n",
        "                                               early_stop,\n",
        "                                               batch_size,\n",
        "                                               EPOCHS,\n",
        "                                               device,\n",
        "                                               desc=desc)\n",
        "\n",
        "            losses['train'].append(t_loss)\n",
        "            losses['val'].append(v_loss)\n",
        "\n",
        "        return losses\n",
        "\n",
        "    def tune_models(\n",
        "            self,\n",
        "            data,\n",
        "            lr=5e-4,\n",
        "            batch_size=16,\n",
        "            EPOCHS=10,\n",
        "            device='cpu'\n",
        "    ):\n",
        "        self.to(device)\n",
        "        losses = []\n",
        "        for model in self.sub_models:\n",
        "            tune_loss = model.tune_model(data, lr, batch_size, EPOCHS, device)\n",
        "            losses.append(tune_loss)\n",
        "\n",
        "        return losses\n",
        "\n",
        "    def get_model_attributes(self):\n",
        "        return {\n",
        "            'n_models': self.n_models,\n",
        "            'seq_length': self.seq_length,\n",
        "            'embed_corpus': self.embed_corpus,\n",
        "            'embed_dim': self.embed_dim,\n",
        "            'n_classes': self.n_classes,\n",
        "            'n_filters': self.n_filters,\n",
        "            'filter_width': self.filter_width,\n",
        "            'pool_size': self.pool_size,\n",
        "            'n_hidden': self.n_hidden,\n",
        "            'dropout': self.dropout,\n",
        "            'rnn_type': self.rnn_type\n",
        "        }\n",
        "\n",
        "    def save_model(self, path):\n",
        "        torch.save((self.get_model_attributes(), self.state_dict()), path)\n",
        "\n",
        "    @classmethod\n",
        "    def load_model(cls, model_path):\n",
        "        model_attr, state_dict = torch.load(model_path)\n",
        "        model = cls(\n",
        "            n_models=model_attr['n_models'],\n",
        "            seq_length=model_attr['seq_length'],\n",
        "            embed_corpus=model_attr['embed_corpus'],\n",
        "            embed_dim=model_attr['embed_dim'],\n",
        "            n_classes=model_attr['n_classes'],\n",
        "            n_filters=model_attr['n_filters'],\n",
        "            filter_width=model_attr['filter_width'],\n",
        "            pool_size=model_attr['pool_size'],\n",
        "            n_hidden=model_attr['n_hidden'],\n",
        "            rnn_type=model_attr['rnn_type'],\n",
        "            dropout=model_attr['dropout']\n",
        "        )\n",
        "        model.load_state_dict(state_dict)\n",
        "\n",
        "        return model\n",
        "\n",
        "\n",
        "class ComponentModel(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            seq_length,\n",
        "            embed_corpus,\n",
        "            embed_dim,\n",
        "            n_classes,\n",
        "            n_filters,\n",
        "            filter_width,\n",
        "            pool_size,\n",
        "            n_hidden,\n",
        "            rnn_type,\n",
        "            dropout\n",
        "    ):\n",
        "        super(ComponentModel, self).__init__()\n",
        "\n",
        "        self.seq_length = seq_length\n",
        "        self.embed_corpus = embed_corpus\n",
        "        self.embed_dim = embed_dim\n",
        "        self.n_classes = n_classes\n",
        "        self.n_filters = n_filters\n",
        "        self.filter_width = filter_width\n",
        "        self.pool_size = pool_size\n",
        "        self.n_hidden = n_hidden\n",
        "        self.rnn_type = rnn_type\n",
        "        self.dropout = dropout\n",
        "\n",
        "        if self.rnn_type is None:\n",
        "            rnn = None\n",
        "        elif self.rnn_type == 'lstm':\n",
        "            rnn = nn.LSTM(self.embed_dim // self.pool_size, self.n_hidden, batch_first=True)\n",
        "        elif self.rnn_type == 'gru':\n",
        "            rnn = nn.GRU(self.embed_dim // self.pool_size, self.n_hidden, batch_first=True)\n",
        "        else:\n",
        "            raise ValueError('Invalid RNN type selected. Options are gru/lstm/None')\n",
        "\n",
        "        self.features = nn.Sequential(OrderedDict([\n",
        "            ('conv1d', nn.Conv1d(self.seq_length, self.n_filters, self.filter_width,\n",
        "                                 padding=(self.filter_width - 1) // 2)),\n",
        "            ('batchnorm', nn.BatchNorm1d(self.n_filters)),\n",
        "            ('relu', nn.ReLU()),\n",
        "            ('maxpool', nn.MaxPool1d(self.pool_size))\n",
        "        ]))\n",
        "\n",
        "        if rnn:\n",
        "            self.features.add_module('rnn', rnn)\n",
        "\n",
        "        self.globalpool = nn.AdaptiveMaxPool1d(self.n_hidden)\n",
        "\n",
        "        self.classifier = nn.Sequential(OrderedDict([\n",
        "            ('linear_h', nn.Linear(self.n_hidden, self.n_hidden // 2)),\n",
        "            ('relu', nn.ReLU()),\n",
        "            ('dropout', nn.Dropout(p=self.dropout)),\n",
        "            ('linear_out', nn.Linear(self.n_hidden // 2, self.n_classes))\n",
        "        ]))\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.rnn_type is None:\n",
        "            x = self.features(x)\n",
        "            x = x.view(-1, 1, self.n_filters * self.embed_dim // self.pool_size)\n",
        "            # h = torch.zeros(x.shape[0], 1, self.n_hidden, device=x.device)\n",
        "        else:\n",
        "            if self.rnn_type == 'lstm':\n",
        "                x, (h, c) = self.features(x)\n",
        "            elif self.rnn_type == 'gru':\n",
        "                x, h = self.features(x)\n",
        "            else:\n",
        "                raise ValueError\n",
        "\n",
        "            x = x.contiguous().view(-1, 1, self.n_filters * self.n_hidden)\n",
        "            h = torch.transpose(h, 0, 1)\n",
        "\n",
        "        x = self.globalpool(x)\n",
        "\n",
        "        # Include hidden state for classifier?\n",
        "        # If so, need to change classifier input shape\n",
        "        # x = torch.cat([x, h], dim=-1)\n",
        "\n",
        "        x = self.classifier(x)\n",
        "\n",
        "        return x.squeeze()\n",
        "\n",
        "    def evaluate(self, data, criterion, batch_size=64, device='cpu'):\n",
        "        data_loader = DataLoader(data, batch_size=batch_size)\n",
        "\n",
        "        y_pred, y_true = [], []\n",
        "        val_loss = 0\n",
        "\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            for X, y in data_loader:\n",
        "                X, y = X.to(device), y.to(device)\n",
        "\n",
        "                logits = self(X)\n",
        "                val_loss += criterion(logits, y).item()\n",
        "\n",
        "                preds = torch.softmax(logits, dim=1).argmax(dim=1)\n",
        "\n",
        "                y_pred.extend(preds.tolist())\n",
        "                y_true.extend(y.tolist())\n",
        "\n",
        "        return y_pred, y_true, (val_loss / len(data_loader))\n",
        "\n",
        "    def train_model(\n",
        "            self,\n",
        "            train_data,\n",
        "            criterion,\n",
        "            optimizer,\n",
        "            val_data=None,\n",
        "            n_samples=1000,\n",
        "            early_stop=False,\n",
        "            batch_size=24,\n",
        "            EPOCHS=20,\n",
        "            device='cpu',\n",
        "            desc=''\n",
        "    ):\n",
        "\n",
        "        best_model, best_epoch = None, -1\n",
        "        val_loss_min = np.Inf\n",
        "\n",
        "        if early_stop:\n",
        "            assert val_data\n",
        "\n",
        "        # If using heuristic bounds and weak loss, use entire training data at each epoch (no class balancing)\n",
        "        train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "        self.to(device)\n",
        "        train_losses, val_losses = [], []\n",
        "        for epoch in tqdm(range(EPOCHS), desc=desc):\n",
        "            self.train()\n",
        "            running_loss = 0\n",
        "\n",
        "            # If using labeled data and cross entropy loss,\n",
        "            # sample training data at each epoch to create a class-balanced training subset\n",
        "            if train_data.labeled:\n",
        "                train_sample = train_data.sample_classes(n_samples=n_samples)\n",
        "                train_loader = DataLoader(train_sample, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "            for X, y in train_loader:\n",
        "                X, y = X.to(device), y.to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                logits = self(X)\n",
        "                loss = criterion(logits, y)\n",
        "\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                running_loss += loss.item()\n",
        "\n",
        "            train_losses.append(running_loss / len(train_loader))\n",
        "\n",
        "            if val_data:\n",
        "                _, _, val_loss = self.evaluate(val_data, criterion, batch_size=batch_size, device=device)\n",
        "                val_losses.append(val_loss)\n",
        "\n",
        "                if early_stop and val_loss <= val_loss_min:\n",
        "                    val_loss_min = val_loss\n",
        "                    best_model = self.state_dict()\n",
        "                    best_epoch = epoch\n",
        "\n",
        "        if early_stop:\n",
        "            self.load_state_dict(best_model)\n",
        "\n",
        "        return train_losses, val_losses\n",
        "\n",
        "    def tune_model(\n",
        "            self,\n",
        "            data,\n",
        "            lr,\n",
        "            batch_size=32,\n",
        "            EPOCHS=20,\n",
        "            device='cpu'\n",
        "    ):\n",
        "        train_loader = DataLoader(data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "        # Freeze feature extraction component\n",
        "        for param in self.features.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        criterion = F.cross_entropy\n",
        "        optimizer = optim.Adam(self.parameters(), lr=lr)\n",
        "\n",
        "        self.to(device)\n",
        "        losses = []\n",
        "        for epoch in tqdm(EPOCHS):\n",
        "            self.train()\n",
        "            running_loss = 0\n",
        "\n",
        "            for X, y in train_loader:\n",
        "                X, y = X.to(device), y.to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                logits = self(X)\n",
        "                loss = criterion(logits, y)\n",
        "\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                running_loss += loss.item()\n",
        "\n",
        "            losses.append(running_loss / len(train_loader))\n",
        "\n",
        "        return losses"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}