{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "preprocess_datasets.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMZoqY+vighXVsUDdZhEmQK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aishik11/DeL-haTE/blob/master/preprocess_datasets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-HM9y3PdKiu"
      },
      "source": [
        "#!/usr/bin/env python3\n",
        "\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "from argparse import ArgumentParser\n",
        "import utils\n",
        "\n",
        "\n",
        "def main():\n",
        "    hon_data = pd.read_csv('data/hon/HON_labeled_data.csv',\n",
        "                           header=0, index_col=0)\n",
        "    hon_data.rename(columns={'class': 'label'}, inplace=True)\n",
        "    hon_data.index = 'h_' + hon_data.index.astype(str)\n",
        "\n",
        "    olid_data = pd.read_csv('data/olid/olid-training-v1.0.tsv',\n",
        "                            sep='\\t', header=0, index_col=0)\n",
        "    olid_data['label'] = olid_data.apply(flatten_labels, axis=1)\n",
        "    olid_data.index = 'o_' + olid_data.index.astype(str)\n",
        "\n",
        "    with open('data/gab/gab_posts_random_sample_100k.json') as f:\n",
        "        gab_data = json.load(f)\n",
        "\n",
        "    stemmer = PorterStemmer()\n",
        "    addl_stops = ['mt', 'rt']\n",
        "\n",
        "    hate_words, offensive_words, positive_words = utils.load_word_lists(stemmer)\n",
        "\n",
        "    print('Processing HON data...')\n",
        "    hon_train, hon_test = process_csv_data(hon_data,\n",
        "                                           stemmer,\n",
        "                                           hate_words,\n",
        "                                           offensive_words,\n",
        "                                           positive_words,\n",
        "                                           stops=addl_stops)\n",
        "\n",
        "    print('Processing OLID data...')\n",
        "    olid_train, olid_test = process_csv_data(olid_data,\n",
        "                                             stemmer,\n",
        "                                             hate_words,\n",
        "                                             offensive_words,\n",
        "                                             positive_words,\n",
        "                                             stops=addl_stops)\n",
        "\n",
        "    combined_train = pd.concat([hon_train, olid_train])\n",
        "    combined_test = pd.concat([hon_test, olid_test])\n",
        "\n",
        "    print('Processing Gab data...')\n",
        "    gab_train = process_gab_data(gab_data,\n",
        "                                 stemmer,\n",
        "                                 hate_words,\n",
        "                                 offensive_words,\n",
        "                                 positive_words,\n",
        "                                 stops=addl_stops)\n",
        "\n",
        "    cols = ['text', 'tokens', 'bounds', 'label']\n",
        "    datasets = {\n",
        "        'hon': (hon_train[cols], hon_test[cols]),\n",
        "        'olid': (olid_train[cols], olid_test[cols]),\n",
        "        'combined': (combined_train[cols], combined_test[cols])\n",
        "    }\n",
        "\n",
        "    # Save processed text, bounds, and label to json\n",
        "    print('Saving datasets to JSON...')\n",
        "    for name, (train, test) in datasets.items():\n",
        "        with open(f'data/{name}/{name}_train.json', 'w') as f:\n",
        "            json.dump(train.to_dict('index'), f, indent=2)\n",
        "        with open(f'data/{name}/{name}_test.json', 'w') as f:\n",
        "            json.dump(test.to_dict('index'), f, indent=2)\n",
        "\n",
        "    with open(f'data/gab/gab_train.json', 'w') as f:\n",
        "        json.dump(gab_train, f, indent=2)\n",
        "\n",
        "\n",
        "def flatten_labels(row):\n",
        "    if row['subtask_a'] == 'NOT':\n",
        "        return 2\n",
        "    elif row['subtask_b'] == 'UNT':\n",
        "        return 1\n",
        "    else:\n",
        "        return 0 if row['subtask_c'] == 'GRP' else 1\n",
        "\n",
        "\n",
        "def process_csv_data(data, stemmer, hate_words, offensive_words, positive_words, stops=None):\n",
        "    # Clean text\n",
        "    data['text'] = data['tweet'].apply(utils.clean_text, stemmer=stemmer, stops=stops)\n",
        "    data['tokens'] = data['text'].apply(word_tokenize)\n",
        "\n",
        "    # Calculate class bounds for weak supervision\n",
        "    data['bounds'] = data['tokens'].apply(utils.calculate_bounds,\n",
        "                                          hate_words=hate_words,\n",
        "                                          offensive_words=offensive_words,\n",
        "                                          positive_words=positive_words)\n",
        "\n",
        "    # Split dataset into train, val, and test sets\n",
        "    test_data = data.sample(frac=args.test_frac)\n",
        "    data.drop(test_data.index, inplace=True)\n",
        "\n",
        "    return data, test_data\n",
        "\n",
        "\n",
        "def process_gab_data(data, stemmer, hate_words, offensive_words, positive_words, stops=None):\n",
        "    placeholders = {'HASHTAGHERE', 'MENTIONHERE'}\n",
        "    clean_data = {}\n",
        "    for post in data:\n",
        "        text = utils.clean_text(post['body'], stemmer, stops=stops)\n",
        "        words = set(word_tokenize(text))\n",
        "\n",
        "        if text and not words.issubset(placeholders):\n",
        "            clean_data[f'g_{post[\"post_id\"]}'] = {\n",
        "                'text': text,\n",
        "                'tokens': word_tokenize(text),\n",
        "                'bounds': utils.calculate_bounds(text, hate_words, offensive_words, positive_words)\n",
        "            }\n",
        "\n",
        "    return clean_data\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = ArgumentParser()\n",
        "\n",
        "    parser.add_argument('--val-frac', type=float, default=0.1,\n",
        "                        help='Fraction of dataset to use as validation set. Default is 0.1.')\n",
        "\n",
        "    parser.add_argument('--test-frac', type=float, default=0.1,\n",
        "                        help='Fraction of dataset to use as test set. Default is 0.1.')\n",
        "\n",
        "    args = parser.parse_args()\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}