{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "utils.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPgzKInvVVake4Z3aUdq68V",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aishik11/DeL-haTE/blob/master/utils.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzctR9Xch96c"
      },
      "source": [
        "#!/usr/bin/env python3\n",
        "\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "from argparse import ArgumentParser\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "import torch\n",
        "from torchtext.vocab import GloVe, FastText, Vectors\n",
        "from dataset import PostDataset\n",
        "\n",
        "\n",
        "def clean_text(text, stemmer, stops=None):\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove urls\n",
        "    www_exp = r'www.[^ ]+'\n",
        "    http_exp = r'https?[^\\s]+'\n",
        "    text = re.sub('|'.join((www_exp, http_exp)), '', text)\n",
        "\n",
        "    # Remove encoded Unicode symbols (removes emoticons etc.)\n",
        "    text = re.sub(r'&.*?;', ' ', text)\n",
        "\n",
        "    # Replace user mentions and hashtags with placeholder\n",
        "    text = re.sub(r'#([\\w\\-]+)', r' HASHTAGHERE \\1', text)\n",
        "    text = re.sub(r'@[\\w\\-]+', r'MENTIONHERE ', text)\n",
        "\n",
        "    # Remove non-letter chars\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
        "\n",
        "    # Remove extra whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    # Remove stop words and apply stemming\n",
        "    nltk_stops = set(stopwords.words('english'))\n",
        "    stops = set(stops).union(nltk_stops) if stops else nltk_stops\n",
        "    text = ' '.join(\n",
        "        stemmer.stem(word)\n",
        "        for word in word_tokenize(text)\n",
        "        if word not in stops\n",
        "    )\n",
        "\n",
        "    # Replace stemmed mention and hashtag placeholders\n",
        "    text = re.sub(r'mentionher', 'MENTIONHERE', text)\n",
        "    text = re.sub(r'hashtagher', 'HASHTAGHERE', text)\n",
        "\n",
        "    return text.strip()\n",
        "\n",
        "\n",
        "def load_word_lists(stemmer):\n",
        "    with open('data/word_lists/hate_words.txt') as f:\n",
        "        hate_words = set(stemmer.stem(word.strip()) for word in f)\n",
        "    with open('data/word_lists/offensive_words.txt') as f:\n",
        "        offensive_words = set(stemmer.stem(word.strip()) for word in f)\n",
        "    with open('data/word_lists/positive_words.txt') as f:\n",
        "        positive_words = set(stemmer.stem(word.strip()) for word in f)\n",
        "\n",
        "    return hate_words, offensive_words, positive_words\n",
        "\n",
        "\n",
        "def load_dataset(dataset, split, embedding, labeled, pad):\n",
        "    with open(f'data/{dataset}/{dataset}_{split}.json') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    return PostDataset(data, embedding, labeled=labeled, padded_seq=pad)\n",
        "\n",
        "\n",
        "def load_embedding(embed_corpus):\n",
        "    corpora = ['glove_twitter', 'glove_commoncrawl', 'fasttext_wiki', 'fasttext_commoncrawl', 'word2vec']\n",
        "    dim = 300\n",
        "\n",
        "    os.makedirs('data/glove', exist_ok=True)\n",
        "    os.makedirs('data/fast_text', exist_ok=True)\n",
        "    os.makedirs('data/word2vec', exist_ok=True)\n",
        "\n",
        "    if embed_corpus == 'glove_twitter':\n",
        "        # GloVe trained on Twitter corpus\n",
        "        embedding = GloVe(name='twitter.27B', dim=200, cache='data/glove/')\n",
        "        dim = 200\n",
        "    elif embed_corpus == 'glove_commoncrawl':\n",
        "        # GloVe trained on Common Crawl corpus\n",
        "        embedding = GloVe(name='42B', dim=300, cache='data/glove/')\n",
        "    elif embed_corpus == 'fasttext_wiki':\n",
        "        # FastText trained on Wikipedia 2017, UMBC webbase corpus and statmt.org news dataset\n",
        "        embedding = FastText(language='en', cache='data/fast_text/')\n",
        "    elif embed_corpus == 'fasttext_commoncrawl':\n",
        "        # FastText trained on Common Crawl corpus\n",
        "        embedding = Vectors(name='crawl-300d-2M.vec',\n",
        "                            url='https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M.vec.zip',\n",
        "                            cache='data/fast_text/')\n",
        "    elif embed_corpus == 'word2vec':\n",
        "        # Word2Vec trained on Google New corpus\n",
        "        name = 'GoogleNews-vectors-negative300.txt'\n",
        "        if os.path.isfile(f'data/word2vec/{name}.pt'):\n",
        "            embedding = Vectors(name=name,\n",
        "                                cache='data/word2vec/')\n",
        "        else:\n",
        "            raise FileNotFoundError(('No torchtext formatted word2vec vectors file found. '\n",
        "                                     'See load_word2vec.py to create the necessary pt file. Requires gensim.'))\n",
        "    else:\n",
        "        raise ValueError(f'Invalid pre-trained word embedding vectors. Options are {\"/\".join(corpora)}.')\n",
        "\n",
        "    return embedding, dim\n",
        "\n",
        "\n",
        "def weak_loss(output, bounds, weight=(1., 1., 1.)):\n",
        "    weight = torch.tensor(weight, device=output.device)\n",
        "\n",
        "    # Convert class scores to probabilities\n",
        "    probs = torch.softmax(output, dim=1)\n",
        "\n",
        "    LBs = bounds[:, :, 0]\n",
        "    UBs = bounds[:, :, 1]\n",
        "\n",
        "    ones = torch.ones_like(probs, device=output.device)\n",
        "\n",
        "    # Loss is applied if output probability falls outside the heuristic bounds\n",
        "    min_lb = torch.min(ones, (ones + probs - LBs))\n",
        "    min_ub = torch.min(ones, (ones + UBs - probs))\n",
        "\n",
        "    nll = -torch.log(min_lb) - torch.log(min_ub)\n",
        "\n",
        "    loss = torch.sum(nll, dim=0)\n",
        "    loss *= weight\n",
        "\n",
        "    # Sum per-class loss\n",
        "    return torch.sum(loss)\n",
        "\n",
        "\n",
        "def calculate_bounds(tokens, hate_words, offensive_words, positive_words):\n",
        "    # TODO: Simplify bounds logic\n",
        "    placeholders = {'HASHTAGHERE', 'MENTIONHERE'}\n",
        "    unique_words = set(tokens) - placeholders\n",
        "    num_unique = len(unique_words)\n",
        "\n",
        "    post_hate = unique_words.intersection(hate_words)\n",
        "    post_offensive = unique_words.intersection(offensive_words) - post_hate\n",
        "    post_positive = unique_words.intersection(positive_words)\n",
        "\n",
        "    num_hate = len(post_hate)\n",
        "    num_offensive = len(post_offensive)\n",
        "    num_positive = len(post_positive)\n",
        "    try:\n",
        "        if post_hate:\n",
        "            hate_LB = min(1., (num_hate + num_offensive) / num_unique)\n",
        "            hate_UB = 1.\n",
        "\n",
        "            offensive_LB = 0.5 * num_offensive / num_unique\n",
        "            offensive_UB = 1 - (num_hate + num_positive) / num_unique\n",
        "\n",
        "            neither_LB = 0.\n",
        "            neither_UB = max(0., 1 - (num_hate + num_offensive) / num_unique)\n",
        "        elif post_offensive:\n",
        "            hate_LB = 0.\n",
        "            hate_UB = 1 - (num_offensive + num_positive) / num_unique\n",
        "\n",
        "            offensive_LB = (num_offensive + 0.5 * num_positive) / num_unique\n",
        "            offensive_UB = 1 - 0.5 * num_positive / num_unique\n",
        "\n",
        "            neither_LB = 0.5 * num_positive / num_unique\n",
        "            neither_UB = 1 - (num_hate + num_offensive) / num_unique\n",
        "        else:\n",
        "            hate_LB = 0.\n",
        "            hate_UB = 1 - num_positive / num_unique\n",
        "\n",
        "            offensive_LB = 0.\n",
        "            offensive_UB = 1 - num_positive / num_unique\n",
        "\n",
        "            neither_LB = num_positive / num_unique\n",
        "            neither_UB = 1.\n",
        "    except ZeroDivisionError:\n",
        "        return (0., 1.), (0., 1.), (0., 1.)\n",
        "\n",
        "    return (hate_LB, hate_UB), (offensive_LB, offensive_UB), (neither_LB, neither_UB)\n",
        "\n",
        "\n",
        "def parse_train_args():\n",
        "    parser = ArgumentParser()\n",
        "\n",
        "    parser.add_argument('dataset', type=str,\n",
        "                        help='Dataset on which to train the model. Options are hon/olid/combined/gab')\n",
        "\n",
        "    parser.add_argument('model_name', type=str,\n",
        "                        help='Model name. The model attributes and state dict will be saved as <model-name>.pt.')\n",
        "\n",
        "    corpora = ['glove_twitter', 'glove_commoncrawl', 'fasttext_wiki', 'fasttext_commoncrawl', 'word2vec']\n",
        "    parser.add_argument('--embed_corpus', type=str, default='glove_commoncrawl',\n",
        "                        help=f'Pre-trained word embedding model/corpus to use. '\n",
        "                             f'Options are {\"/\".join(corpora)}. Default is glove_commoncrawl')\n",
        "\n",
        "    parser.add_argument('--pad', default=50,\n",
        "                        help='Length of padded input to pass to the model. Default is 50.')\n",
        "\n",
        "    parser.add_argument('--n_models', type=int, default=5,\n",
        "                        help='Number of models in the ensemble. Default is 5.')\n",
        "\n",
        "    parser.add_argument('--n_filters', type=int, default=32,\n",
        "                        help='Number of filters in the CNN layer. Default is 32.')\n",
        "\n",
        "    parser.add_argument('--filter_width', type=int, default=5,\n",
        "                        help='Width of each filter in the CNN layer. Default is 5.')\n",
        "\n",
        "    parser.add_argument('--pool_size', type=int, default=4,\n",
        "                        help='Kernel size for max pooling. Default is 4.')\n",
        "\n",
        "    parser.add_argument('--n_hidden', type=int, default=100,\n",
        "                        help='Size of the hidden layers. Default is 100.')\n",
        "\n",
        "    parser.add_argument('--rnn_type', type=str, default='gru',\n",
        "                        help='Type of RNN to use in the model. Options are gru/lstm/None. Default is gru')\n",
        "\n",
        "    parser.add_argument('--dropout', type=float, default=0.2,\n",
        "                        help='Dropout probability during model training. Default is 0.2.')\n",
        "\n",
        "    parser.add_argument('--use_val', default=True, action='store_true',\n",
        "                        help='Default True. Use validation data to evaluate model after each training epoch.')\n",
        "\n",
        "    parser.add_argument('--early_stop', default=False, action='store_true',\n",
        "                        help='Default False. Use early stopping during training to save model state at the epoch '\n",
        "                             'with minimum validation loss.')\n",
        "\n",
        "    parser.add_argument('--use_weak_loss', dest='weak_loss', default=False, action='store_true',\n",
        "                        help='Default False. Use heuristic class bounds and weak loss for training. '\n",
        "                             'When False, use true class labels and cross entropy loss.')\n",
        "\n",
        "    parser.add_argument('--class_weight', type=float, default=[1., 1., 1.], nargs='+',\n",
        "                        help='Per-class weight for weak loss calculation. Default (1., 1., 1.).')\n",
        "\n",
        "    parser.add_argument('--learn_rate', type=float, default=1e-3,\n",
        "                        help='Learning rate for Adam optimizer during training. Default is 1e-3.')\n",
        "\n",
        "    parser.add_argument('--n_samples', default=1000,\n",
        "                        help='Number of observations to sample from each class at each training epoch. '\n",
        "                             'May be an integer or \"all\" to set sample size equal to the number of observations '\n",
        "                             'in the smallest class. Default is 1000.')\n",
        "\n",
        "    parser.add_argument('--batch_size', type=int, default=32,\n",
        "                        help='Batch size to use during training. Default is 32.')\n",
        "\n",
        "    parser.add_argument('--epochs', type=int, default=20,\n",
        "                        help='Number of training epochs. Default is 20.')\n",
        "\n",
        "    parser.add_argument('--use_gpu', type=bool, default=True,\n",
        "                        help='Use GPU for model training. Default is True.')\n",
        "\n",
        "    return parser.parse_args()\n",
        "\n",
        "\n",
        "def plot_loss(dataset, train_losses, val_losses, early_stop=False):\n",
        "    fig = plt.figure(figsize=(12, 8))\n",
        "    plt.plot(train_losses, label='Training loss')\n",
        "\n",
        "    if val_losses:\n",
        "        plt.plot(val_losses, label='Validation loss')\n",
        "        if early_stop:\n",
        "            plt.vlines(val_losses.index(min(val_losses)),\n",
        "                       min([*train_losses, *val_losses]), max([*train_losses, *val_losses]),\n",
        "                       linestyles='dashed')\n",
        "\n",
        "    plt.legend(frameon=False)\n",
        "    plt.title(f'Model training loss on {dataset.upper()} data')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Avg. Loss per Batch')\n",
        "    plt.show()\n",
        "    plt.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}